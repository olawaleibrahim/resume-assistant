{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_google_community import DocAIParser\n",
    "from langchain_core.document_loaders.blob_loaders import Blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "REGION = os.getenv(\"REGION\")\n",
    "INDEX_NAME = os.getenv(\"INDEX_NAME\")\n",
    "INDEX_ID = os.getenv(\"INDEX_ID\")\n",
    "DB_USER= os.getenv(\"CLOUD_SQL_USER1\")\n",
    "DB_PASS = os.getenv(\"CLOUD_SQL_PASSWORD\")\n",
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "DOC_PROCESSOR_NAME = os.getenv(\"DOC_PROCESSOR_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_webpage_content(url):\n",
    "    \"\"\"Scrapes the entire content of the given webpage.\"\"\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page, status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    page_content = soup.get_text(separator='\\n', strip=True)\n",
    "    \n",
    "    return page_content\n",
    "\n",
    "\n",
    "def get_json_from_result(result):\n",
    "    return json.loads(result.content[8:-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.linkedin.com/jobs/view/4157337860\",\n",
    "    \"https://job-boards.greenhouse.io/scaleai/jobs/4413274005\",\n",
    "    \"https://www.linkedin.com/jobs/view/4091428817\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that helps to extract the job description from a corpus of words from a webpage\"\n",
    "                \"\"\"\n",
    "                You analyse the content provide, and extract only the job requirements and responsibilities.\n",
    "                The content contains other irrelevant text extracted from the web page. Your job is to output only \n",
    "                the job requirements and responsibilities, skills, company. Do not modify the original words in the job description.\n",
    "                Return the extracted job description only as a json object.\n",
    "                Example:\n",
    "                {\n",
    "                    \"job_description\": \"This is a job that does nothing\"\n",
    "                }\n",
    "                \"\"\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Kindly extract the job description from this: {webpage_content}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ats_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=(\n",
    "                \"You are a helpful assistant that helps review resumes for ATS compliance\"\n",
    "                \"\"\"You ensure that you use the provided job description to make the resume more ATS friendly \\n\n",
    "                Do not remove quantifiable metrics found in the resume experience, but only modify the resume to include the keywords found in the job_description \n",
    "                Ensure that the work experience essence is not lost in the resume content is retained while modifying the sentences to semnatically include keywords in the job description\\n\n",
    "                Provide only content that should go into the final resume and nothing extra\"\"\"\n",
    "                \"\"\"You always respond in a structured format with the following subheadings: \\n\n",
    "                1. Summary \\n\n",
    "                2. Experience \\n\n",
    "                3. Skills \\n\n",
    "                4. Missing Keywords \\n\n",
    "                5. In this section only, list changes made to the resume to make it ATS friendly that was previously missing \\n\n",
    "                \"\"\"\n",
    "            )\n",
    "        ),\n",
    "        HumanMessagePromptTemplate.from_template(\n",
    "            \"Make my resume and work experience more ATS friendly: \\n {resume_content} \\n using the job's description: \\n {job_description}\"\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_description(chat_template, web_url):\n",
    "\n",
    "    webpage_content = scrape_webpage_content(web_url)\n",
    "    chat_model = ChatVertexAI(model_name=\"gemini-1.5-pro\")\n",
    "    chain = chat_template | chat_model \n",
    "    result = chain.invoke({\"webpage_content\": webpage_content})\n",
    "    \n",
    "    content_json = get_json_from_result(result)\n",
    "    return content_json[\"job_description\"]\n",
    "\n",
    "\n",
    "def get_resume_content(filename=\"Olawale_Machine_Learning_Engineer_Template.pdf\"):\n",
    "    parser = DocAIParser(location=\"us\",\n",
    "                     processor_name=DOC_PROCESSOR_NAME,\n",
    "                     gcs_output_path=\"gs://{}/resume-assistant/data/output/dev/pdfs\".format(GCS_BUCKET_NAME))\n",
    "    inp_path = f\"gs://{GCS_BUCKET_NAME}/resume-assistant/data/input/dev/pdfs/{filename}\"\n",
    "    blob = Blob(path=inp_path)\n",
    "    docs = list(parser.lazy_parse(blob))\n",
    "    resume_content = \"\"\n",
    "    for doc in docs:\n",
    "        resume_content+=doc.page_content\n",
    "\n",
    "    return resume_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job1 = get_job_description(chat_template, urls[0])\n",
    "print(job1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_content = get_resume_content()\n",
    "print(resume_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatVertexAI(model_name=\"gemini-1.5-pro\")\n",
    "ats_chain = ats_template | chat_model \n",
    "ats_result = ats_chain.invoke({\"job_description\": job1,\n",
    "                       \"resume_content\": resume_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ats_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_bucket(blob_name, path_to_file, bucket_name):\n",
    "    \"\"\" Upload data to a bucket\"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(blob_name)\n",
    "    blob.upload_from_filename(path_to_file)\n",
    "\n",
    "    return blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_to_bucket(blob_name=\"filename.pdf\",\n",
    "                 path_to_file=\"\",\n",
    "                 bucket_name=GCS_BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# Instantiates a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# The name for the new bucket\n",
    "bucket_name = \"my-new-bucket\"\n",
    "\n",
    "# Creates the new bucket\n",
    "bucket = storage_client.create_bucket(bucket_name)\n",
    "\n",
    "print(f\"Bucket {bucket.name} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", style=\"B\", size=16)\n",
    "        self.cell(200, 10, \"Professional Resume\", ln=True, align='C')\n",
    "        self.ln(10)\n",
    "    \n",
    "    def section_title(self, title):\n",
    "        self.set_font(\"Arial\", style=\"B\", size=14)\n",
    "        self.cell(0, 10, title, ln=True, align='L')\n",
    "        self.ln(5)\n",
    "    \n",
    "    def section_body(self, text):\n",
    "        self.set_font(\"Arial\", size=12)\n",
    "        self.multi_cell(0, 10, text)\n",
    "        self.ln(5)\n",
    "\n",
    "def save_to_pdf(content, filename=\"resume.pdf\"):\n",
    "    \"\"\"Saves structured content to a PDF file.\"\"\"\n",
    "    pdf = PDF()\n",
    "    pdf.set_auto_page_break(auto=True, margin=15)\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # Adding sections\n",
    "    pdf.section_title(\"Professional Summary\")\n",
    "    pdf.section_body(\"Experienced professional with a strong background in machine learning and software engineering.\")\n",
    "    \n",
    "    pdf.section_title(\"Work Experience\")\n",
    "    pdf.section_body(content)  # Assuming scraped content is relevant job experience\n",
    "    \n",
    "    pdf.section_title(\"Skills\")\n",
    "    pdf.section_body(\"- Python\\n- Machine Learning\\n- Data Science\\n- Web Scraping\\n- Distributed Computing\")\n",
    "    \n",
    "    pdf.output(filename)\n",
    "    print(f\"Content saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_pdf(ats_result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def replace_text_in_pdf(input_pdf, output_pdf, search_text, replace_text):\n",
    "    doc = fitz.open(input_pdf)\n",
    "    \n",
    "    for page in doc:\n",
    "        text_instances = page.search_for(search_text)\n",
    "        print(\"text_instances\", len(text_instances), text_instances)\n",
    "        \n",
    "        if text_instances:\n",
    "            page.add_redact_annot(text_instances[0], replace_text, fontname=\"helv\", fontsize=11)\n",
    "            page.apply_redactions(images=fitz.PDF_REDACT_IMAGE_NONE) \n",
    "    \n",
    "    doc.save(output_pdf)\n",
    "    doc.close()\n",
    "    print(f\"Modified PDF saved as {output_pdf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_text = \"\"\"\n",
    "CI/CD pipelines implemented using GitHub actions to ensure seamless continuous integration\n",
    "and development, resulting in ~50% reduction in production downtimes\n",
    "\"\"\"\n",
    "new_text = \"\"\"\n",
    "Developed and productionised automated ML backend service with django backend for computer\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace_text_in_pdf(file_path, output_path, \"SOTA\", \"new_text\")\n",
    "replace_text_in_pdf(file_path, output_path, prev_text, new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/olawale/Desktop/PROJECTS/llms/LLMS/resume-assistant/data/input/dev/pdfs/Olawale_Machine_Learning_Engineer_Template.pdf\"\n",
    "output_path = \"/home/olawale/Desktop/PROJECTS/llms/LLMS/resume-assistant/data/input/dev/pdfs/Olawale_Machine_Learning_Engineer_Template_output.pdf\"\n",
    "output_docx = \"/home/olawale/Desktop/PROJECTS/llms/LLMS/resume-assistant/data/input/dev/pdfs/Olawale_Machine_Learning_Engineer_Template_output.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pdflatex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypandoc\n",
    "def convert_docx_to_pdf(input_docx, output_pdf):\n",
    "    pypandoc.convert_file(input_docx, 'latex', outputfile=output_pdf)\n",
    "    print(f\"Converted {input_docx} to {output_pdf}\")\n",
    "\n",
    "# Example usage\n",
    "convert_docx_to_pdf(output_docx, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2docx import Converter\n",
    "\n",
    "def convert_pdf_to_docx(input_pdf, output_docx):\n",
    "    cv = Converter(input_pdf)\n",
    "    cv.convert(output_docx, start=0, end=None)  # Convert all pages\n",
    "    cv.close()\n",
    "    print(f\"Converted {input_pdf} to {output_docx}\")\n",
    "\n",
    "# Example usage\n",
    "convert_pdf_to_docx(file_path, output_docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file and returns it as a string.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() + \"\\n\"  # Extract text from each page\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = extract_text_from_pdf(file_path)\n",
    "print(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = Blob(data=pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/olawale/Desktop/PROJECTS/llms/LLMS/resume-assistant/filename.txt\", \"r\") as file:\n",
    "    data_read = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_encode = data_read.encode(\"utf8\").split(b\";base64,\")[1]\n",
    "data_decode = base64.decodebytes(data_encode)\n",
    "data_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "# storage_client = storage.Client()\n",
    "# bucket = storage_client.get_bucket(GCS_BUCKET_NAME)\n",
    "# blob = bucket.blob(\"resume-assistant/data/output/dev/pdfs/test_bytes.pdf\")\n",
    "# blob.upload_from_string(pdf_text, content_type=\"application/pdf\")\n",
    "blob.public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from resume_assistant.application.dataset.upload import upload_to_gcs\n",
    "\n",
    "upload_to_gcs(\"trial.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
