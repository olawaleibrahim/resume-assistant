{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from resume_assistant.application.dataset.upload import upload_to_gcs\n",
    "from resume_assistant.application.rag.retriever import get_langchain_docs\n",
    "from resume_assistant.application.rag.utils import extract_content_from_documents\n",
    "from resume_assistant.application.dataset.extraction import extract_job_description, scrape_webpage_content\n",
    "from resume_assistant.models.model import get_model\n",
    "import resume_assistant.application.rag.templates as templates \n",
    "import resume_assistant.application.processing.postprocess as postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = None   # used for local testing\n",
    "user_resume_path = \"ML Engineer Example.pdf\"   # file relative directory: ./resume-assistant/data/input/dev/pdfs\n",
    "url = \"https://job-boards.greenhouse.io/scaleai/jobs/4413274005\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data ingestion to GCP Cloud storage and indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root_dir /home/olawale/Desktop/PROJECTS/llms/resume-assistant\n"
     ]
    }
   ],
   "source": [
    "public_url = upload_to_gcs(user_resume_path, content)\n",
    "documents = get_langchain_docs(user_resume_path)\n",
    "content = extract_content_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_google_community import DocAIParser\n",
    "from langchain_core.document_loaders.blob_loaders import Blob\n",
    "\n",
    "\n",
    "GCS_BUCKET_NAME = os.getenv(\"GCS_BUCKET_NAME\")\n",
    "DOC_PROCESSOR_NAME = os.getenv(\"DOC_PROCESSOR_NAME\")\n",
    "\n",
    "parser = DocAIParser(location=\"us\",\n",
    "                    processor_name=DOC_PROCESSOR_NAME,\n",
    "                    gcs_output_path=\"gs://{}/resume-assistant/data/output/dev/pdfs\".format(GCS_BUCKET_NAME))\n",
    "inp_path = f\"gs://{GCS_BUCKET_NAME}/resume-assistant/data/input/dev/pdfs/{user_resume_path}\"\n",
    "blob = Blob(path=inp_path)\n",
    "operations = parser.docai_parse([blob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while parser.is_running(operations):\n",
    "    time.sleep(0.5)\n",
    "results = parser.get_results(operations)\n",
    "docs = list(parser.parse_from_results(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web/Job page scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '{\"error\":\"Forbidden\"}\\n')\n",
      "Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError('403 Client Error: Forbidden for url: https://api.smith.langchain.com/runs/batch', '{\"error\":\"Forbidden\"}\\n')\n"
     ]
    }
   ],
   "source": [
    "webpage_content = scrape_webpage_content(url)\n",
    "job_description = extract_job_description(webpage_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = get_model()\n",
    "ats_chain = templates.ATS_TEMPLATE | chat_model \n",
    "ats_result = ats_chain.invoke({\"job_description\": job_description,\n",
    "                       \"resume_content\": content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_ats_result = postprocess.get_json_from_result(ats_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Summary', 'Experience', 'Skills', 'Missing Keywords', 'Changes Made'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_ats_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2. Machine Learning Engineer, Earth Science Analytics, Athens, Greece (February 2022 - December 2022)', '    - Developed and productionized an automated ML backend service with a Django backend for computer vision workflows using SOTA models like YOLO, Mask RCNN, and ResNet, achieving a 90-95% reduction in manpower hours.', '    - Deployed training and prediction APIs and endpoints for 3D CNN architectures (AttentionUnet, TransUnet, Unet+++, ResUnet) for self-supervised machine learning denoising of seismic images, saving 100% of the time required for manual labeling.', '    - Developed 3D rock property prediction machine learning models using 1D CNN architectures.', '    - Productionized pretrained segmentation and denoising models in EarthNET and EarthVision for clients.', '3. Machine Learning Developer, dGB Earth Sciences, Enschede, Netherlands (March 2021 - February 2022)', '    - Built and integrated a PyTorch machine learning plugin for computer vision workflows into C++ software, enhancing support and improving automated interpretation for SMEs.', '    - Developed computer vision segmentation models for interpreting various seismic and well-log data, utilizing 2D and 3D CNN models in PyTorch, TensorFlow, and classical ML algorithms in scikit-learn.']\n"
     ]
    }
   ],
   "source": [
    "print(json_ats_result[\"Experience\"][6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
